# 图神经网络模型后门检测

## 后门攻击

backdoored model会将带有特定trigger（在图数据结构中一般指特殊的子图结构）的图分类为攻击者指定的类别；一般来说，攻击者要达到的效果是，只要将trigger附着在输入图上，就会导致模型的误分类。

## 目标

判断一个用于图分类的模型是否拥有后门，如果有，指出攻击者的目标攻击类（target class）。

## 假设

1）黑箱模型，只能获取模型的分类置信度（softmax结果），无法得到梯度。

2）拥有一个干净的数据集，其中有所有类别的数据。

## 思想

1）修改每一个样本$(g_i,y_j)$，使模型将修改后的样本误分类为$y_k(k{\neq}j)$；一次修改包含多步，每一步修改一条边，记录所有的修改过程，一次修改的总和即为还原的触发器；对每一个样本共进行$n-1$次修改，得到能够将该样本误分类为其他$n-1$类的触发器。

2）对于每一对标签$y_i{\rightarrow}y_j$，根据步骤1）会生成一个触发器集合，从触发器的大小、密度或者修改前后的图间距离可以得到一个多维的度量值，对所有标签对的度量值进行异常检测（检测异常小），以此为依据判断clean/dirty模型；并且，若$y_i{\rightarrow}y_j$度量值异常小，则$y_j$为攻击者所要攻击的标签。

## 方法

使用图上的强化学习方法。

相比于基于强化学习的图解释方法，这里的action space要大得多，因为解释器只需要将图中原有的边作为action space；而如果要生成触发器，action space是完全图的所有边。

基于这个前提，将每一个选取边的action分解为选取两个节点，可以将复杂度从$O(|V|^2)$降低到$O(2|V|)$。


